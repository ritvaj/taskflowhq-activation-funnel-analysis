<h1 align="center">TaskFlowHQ: Diagnosing Drop-Off & Hesitation Before Activation</h1>



## Overview

This project examines how new users move from initial exploration to meaningful usage in a B2B SaaS product, and where that journey breaks down.

TaskFlowHQ is a simulated B2B SaaS productivity tool designed to help individuals and teams plan, track, and complete work through structured tasks and workflows.

This analysis focuses on new-user onboarding and their activation behavior using GA4 event data in BigQuery (GCP), diagnosing activation drop-off and post-feature discovery hesitation, and examining how the two interact to suppress conversion and early retention.

---

## Business Problem

This analysis is motivated by a common SaaS challenge: users enter the product but fail to convert early interest into sustained value.

Despite strong top-of-funnel entry, a large proportion of users fail to complete their first meaningful action, limiting:

- user retention  
- product adoption  
- long-term revenue potential  

Activation represents the point at which users are expected to experience real value. However, failure to reach this stage can arise from multiple causes, including onboarding friction, unclear value communication, or user hesitation and decision uncertainty. The relative impact of these factors is unclear.

This project focuses on identifying where activation breakdown occurs and isolating the primary drivers behind it to inform focused, testable product improvements.

---

## Objectives

These objectives define the key questions guiding the analysis and ensure the work stays focused on real product decisions rather than open-ended exploration.

This project aims to:

- Identify where and why users drop off before reaching activation.
- Distinguish between structural friction (users being blocked by onboarding or navigation) and behavioral friction (users hesitating despite being able to proceed).
- Measure how long users take to move between key steps, not just whether they convert.
- Use segmentation and A/B testing to validate hypotheses, not just describe patterns.
- Isolate the primary root causes behind activation failure based on the observed data.
- Translate insights into practical, testable product recommendations.

---

## Analysis Workflow

The workflow below outlines how raw GA4 event data was translated into a B2B SaaS activation diagnosis and product recommendations:

1. Started with raw GA4 event-level tables in BigQuery to understand what user action events were available.
2. Audited and validated events, then mapped GA4 events to B2B SaaS user actions (app open, onboarding, feature discovery, activation).
3. Constructed a user-level activation funnel by anchoring each step to the first valid event timestamp after the previous step.
4. Measured conversion and drop-off at each funnel stage to identify where users disengage.
5. Analyzed time-to-next-step metrics to detect post-onboarding hesitation among users who discovered a feature or activated.
6. Evaluated early retention for the onboarding cohort to assess whether unresolved activation friction leads to weak habit formation and early churn.
7. Segmented activated users by activation speed to separate usability issues from decision friction.
8. Simulated A/B testing framework to demonstrate how post-discovery improvements could be validated.
9. Created supporting visuals to clearly communicate funnel behavior, hesitation patterns, retention decay, and experimental outcome.
10. Conducted root cause analysis and translated findings into actionable, testable product recommendations.

---

## User Journey, Funnel Definition and Event Mapping

This section clarifies how a new user experiences TaskFlowHQ and how abstract behavior is translated into measurable funnel steps for analysis.

### User Journey

A typical new user flow looks like this:

1. User opens TaskFlowHQ  
2. Proceeds through onboarding or setup screens  
3. Explores at least one core feature  
4. Attempts to complete a task or workflow  
5. Successfully completes a first meaningful action (activation)

This sequence represents the minimum path required for a user to experience and understand the productâ€™s value.

### Activation Definition

Activation is defined as the point at which a user moves from passive exploration to intentional use.
At this stage, the user demonstrates that TaskFlowHQ can be applied to a real task, rather than being casually browsed or tested.

### Funnel Steps

The user journey is operationalized into the following measurable funnel steps:

| Step | Description |
|---|---|
| App Open | User opens the application |
| Onboarding Viewed | Initial setup or login screens |
| Feature Discovery | User explores a core feature |
| Activation | User completes first meaningful action |

**Funnel refinement note:**  
An intermediate â€œStart Core Actionâ€ step was initially considered. Exploratory analysis showed that almost no users meaningfully passed through this step, and it did not provide additional behavioral signal. The step was removed to ensure the funnel reflects observed user behavior rather than assumed intent.

#### GA4 Event Mapping (Reference)

The table below shows how raw GA4 events were mapped to product-level funnel steps used in the analysis.

| GA4 Event | Product Interpretation |
|----------|------------------------|
| session_start | App Open |
| page_view (login / home) | Onboarding |
| view_item | Feature Discovery |
| purchase | Activation |

---

## Data & Tools

### Tools Used
- **BigQuery** (GA4 event-level data)  
- **SQL** (funnel construction, retention cohort, segmentation, experiment logic)  
- **Python (Matplotlib)** for custom visualizations  

### Skills Applied
- Product analytics  
- Funnel and drop-off analysis  
- Time-to-event analysis  
- Retention cohort analysis  
- Segmentation (Fast vs Slow Activators)  
- A/B testing design and statistical evaluation  
- Root cause analysis (RCA)

---

## KPI Framework

This section defines the core metrics used to evaluate activation performance, user friction, and early retention behavior.

### Activation & Funnel Loss

**Step-level drop-off rates:**
- App Open â†’ Onboarding: 74.2% drop-off
- Onboarding â†’ Feature Discovery: 66.7% drop-off
- Feature Discovery â†’ Activation: 95.5% drop-off

**Overall activation and progression rates:**
- **Feature Discovery Reach:** 33.3%
- **Activation Completion:** 4.5%  

These metrics highlight a sharp drop between feature discovery and activation, which becomes the primary focus of deeper analysis in later sections.

### Pacing & Friction

**Median time between key steps:**
- Median time (Onboarding â†’ Feature Discovery): **74s**  
- Median time (Feature Discovery â†’ Activation): **727s (~12 min)**

The substantial increase in time after feature discovery indicates elevated friction during the final activation step. Time-based patterns are analyzed further to distinguish structural barriers from user hesitation.

### Retention (Onboarding Cohort)

Short-term retention rates for users who reached onboarding:
- Day 1: **8.5%**  
- Day 3: **2.8%**  
- Day 7: **0.8%**

Retention declines sharply within the first few days, suggesting weak early habit formation and limited short-term stickiness following initial product exposure.

---

## Segmentation: Activation Speed

To understand whether post-discovery friction reflects usability constraints or user hesitation, activated users were segmented based on time-to-activation.

Activated users were split using the median Feature Discovery â†’ Activation time (727 seconds) as the threshold:
- Fast Activators: Activated in â‰¤ 727 seconds
- Slow Activators: Activated in > 727 seconds

| Segment | Users | Share |
|---|---|---|
| Fast Activators | 4 | 44% |
| Slow Activators | 5 | 56% |

**Key Insight:**  
Key Insight:
Slow Activators do not exhibit difficulty with onboarding or navigation. Their friction emerges after feature discovery, indicating hesitation rather than inability to proceed.

The prolonged delay among users who eventually activate suggests that the same post-discovery friction may also be contributing to abandonment among non-activating users, making this stage a high-leverage point for intervention.

---

## A/B Testing: Validating Post-Discovery Hesitation

This section demonstrates how experimentation logic will be used to evaluate whether reinforcing value after feature discovery reduces activation friction.

### Hypothesis

- **Hypothesis:** Reinforcing value post-discovery improves activation and reduces hesitation  
- **Null Hypothesis:** No difference between variants  

### Experiment Setup

Users were deterministically split for analytical demonstration:

- **Variant A:** Reinforced value after feature discovery  
- **Variant B:** Baseline experience  

In production, users would be randomly assigned over a fixed exposure window.

### Results

**Primary metric: Activation rate**

- Ï‡Â² = 0.087, df = 1  
- p-value = 0.77  

With only 9 total activations, the test is underpowered and the null hypothesis is not rejected.

**Secondary metric: Time to activation**

| Metric | Variant A | Variant B |
|---|---:|---:|
| Activation Rate | 4.9% | 4.0% |
| Median Time to Activation | 6.3 min | 12.2 min |

Variant A shows a substantially lower median time to activation, indicating reduced post-discovery hesitation among users who do convert.

### Decision

Activation lift is inconclusive due to small sample size, but the reinforced experience shows directionally faster activation. This variant is worth iterating and re-testing with larger exposure, focusing on clearer next actions after feature discovery.

---

## Root Cause Analysis (RCA)

This section isolates the underlying cause of activation drop-off using observed user behavior.

### Primary Finding

Activation friction is driven by: a sharp drop-off between feature discovery and activation, and prolonged hesitation among users who do eventually activate.
**Hypothesis**: The same post-discovery hesitation that delays activation for some users is also causing many others to drop off entirely, contributing directly to early churn.

Retention analysis reinforces the severity of this friction. Users who fail to activate quickly also fail to return, with onboarding cohort retention dropping to 8.5% on Day 1 and below 1% by Day 7. This indicates that unresolved post-discovery hesitation does not merely delay activation, but contributes directly to early disengagement and weak habit formation.

### What the Data Shows
- Users reach onboarding and feature discovery reliably  
- Activation often occurs, but **after long delays**  
- Fast and Slow Activators differ mainly in **decision speed**, not capability  

### What This Is NOT
- Not an onboarding usability problem  
- Not a navigation issue  
- Not a discoverability failure  

**Root Cause:**  
A **decision-confidence and value-clarity gap** immediately after feature discovery.

---

## Recommendations

These recommendations target the post-discovery friction identified in the analysis, where sharp drop-off and prolonged hesitation converge, and are ordered by expected impact.
Because post-discovery hesitation both suppresses activation and accelerates early churn, interventions at this stage have disproportionate impact. Improving value clarity and decision confidence immediately after feature discovery addresses not only conversion loss, but also weak early retention.

### Priority 1: Reinforce Value Immediately After Discovery
- Explicitly communicate outcomes and benefits  
- Focus on value, not features  

### Priority 2: Reduce Perceived Commitment
- Signal low effort and reversibility  
- Reduce perceived risk at the activation moment  

### Priority 3: Clarify the Immediate Next Step
- Provide a single, explicit recommended action  
- Reduce ambiguity between discovery and execution  

All recommendations are incremental, testable through controlled experiments, and directly target the post-discovery friction identified in the analysis.

---

## Visuals

*Note: Funnel counts were visualized directly in BigQuery to reflect raw event-level aggregation, while pacing, retention, and experimentation visuals were created using Python for finer control.*
All generated figures are available in the [`visuals/`](visuals) directory.


### Funnel Overview â€” Drop-off Concentration
<img src="visuals/funnel_overview.png" width="800"><br><br>

**Insight:**  
Drop-off accelerates sharply after onboarding and feature discovery, with a steep contraction before activation. This shows that users are entering the product and exploring features but failing to convert exploration into committed usage, pointing to an activation clarity issue rather than an acquisition or onboarding failure.

---

### Median Time-to-Next-Step â€” Post-Discovery Hesitation
<img src="visuals/median_time_to_next_step.png" width="800"><br><br>

**Insight:**  
Users move quickly from onboarding to feature discovery, but the median time from discovery to activation is disproportionately long. This gap signals hesitation after initial exploration, suggesting uncertainty about next steps or insufficient confidence in the productâ€™s immediate value.

---

### Onboarding Cohort Retention (D1 / D3 / D7) â€” Weak Early Habit Formation
<img src="visuals/onboarding_retention_d1_d3_d7.png" width="800"><br><br>

**Insight:**  
Retention declines sharply across Day 1, Day 3, and Day 7, with fewer than 1% of onboarded users returning by Day 7. This indicates that most users disengage before forming an early habit, reinforcing the need to accelerate value realization immediately after onboarding.

---

### A/B Activation Comparison â€” Correct Experiment Interpretation
<img src="visuals/ab_activation_horizontal_bar.png" width="800"><br><br>

**Insight:**  
Activation conversion differs only marginally between Variant A and Variant B and is not statistically significant. Given the absence of a real product intervention and limited sample size, this result is expected and demonstrates correct experimental interpretation rather than over-reading random variation.

### Final Insight

Users are able to reach onboarding and feature discovery, but a sharp drop-off occurs immediately afterward. Among the small group who do activate, long delays between feature discovery and activation point to hesitation rather than usability or navigation issues. 
Early retention also falls rapidly after onboarding, reinforcing that unresolved activation friction prevents users from forming an initial habit. 
Together, the funnel drop-off, prolonged activation delays, and weak early retention indicate that post-discovery decision uncertainty drives both delayed activation and outright abandonment.

---

## Future Work / Next Steps

If TaskFlowHQ were a live product, this analysis would be extended in the following ways:

- Run controlled experiments to test post-discovery value reinforcement and measure impact on both activation rate and time-to-activation.  
- Evaluate whether faster activation causally improves early retention by comparing retention curves for activated vs non-activated users.
- Expand retention analysis beyond a single onboarding cohort to rolling and feature-based cohorts.
- Instrument missing or ambiguous core-action events to improve activation measurement fidelity. 
- Increase experiment exposure and sample size to enable statistically powered A/B testing and reliable inference.

---

## References

- Google Analytics 4 BigQuery Export Schema  
  https://support.google.com/analytics/answer/7029846

- Google BigQuery SQL Documentation  
  https://cloud.google.com/bigquery/docs/reference/standard-sql

---

## ðŸ‘¤ About Me

Iâ€™m **Ritvaj Madotra**, a data analyst passionate about using **Python, SQL, and business analytics** to design interpretable, impact-driven solutions.  
ðŸ“Œ Connect: [LinkedIn](https://www.linkedin.com/in/ritvajmadotra) | [GitHub](https://github.com/ritvaj)
